{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook 00: Setup & Verification\n",
        "\n",
        "\n",
        "**Prerequisites:**\n",
        "- Install dependencies: `pip install -r requirements.txt` or `uv sync`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key Status:\n",
            "--------------------------------------------------\n",
            "OpenAI               Found      (...esp_VQoA)\n",
            "Google Gemini        Found      (...ukLZ55NY)\n",
            "Groq                 Found      (...L0uHiohh)\n"
          ]
        }
      ],
      "source": [
        "# Setup imports and environment\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys\n",
        "providers = {\n",
        "            \"OpenAI\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "            \"Google Gemini\": os.getenv(\"GEMINI_API_KEY\"),\n",
        "            \"Groq\": os.getenv(\"GROQ_API_KEY\"),\n",
        "            }\n",
        "\n",
        "print(\"API Key Status:\")\n",
        "print(\"-\" * 50)\n",
        "for provider, key in providers.items():\n",
        "    status = \"Found\" if key else \"Missing\"\n",
        "    preview = f\"(...{key[-8:]})\" if key else \"\"\n",
        "    print(f\"{provider:20s} {status:10s} {preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Utilities\n",
        "\n",
        "Import our custom utilities for token counting, logging, and LLM client abstraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available prompts: 11\n",
            "Prompts: skeleton.v1, zero_shot.v1, few_shot.v1, cot_reasoning.v1, tot_reasoning.v1, json_extract.v1, tool_call.v1, overflow_summarize.v1, rate_limit_retry.v1, style_persona.v1, router_classify.v1\n"
          ]
        }
      ],
      "source": [
        "from utils.prompts import render, PROMPTS, list_prompts\n",
        "from utils.llm_client import LLMClient\n",
        "from utils.logging_utils import log_llm_call, get_log_summary\n",
        "from utils.router import pick_model\n",
        "from utils.token_utils import count_messages_tokens, reconcile_usage\n",
        "\n",
        "print(f\"Available prompts: {len(list_prompts())}\")\n",
        "print(f\"Prompts: {', '.join(list_prompts())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Hello World with Token Tracking\n",
        "\n",
        "Let's send a simple hello-world request to each provider and compare:\n",
        "- **Estimated tokens** (via tiktoken)\n",
        "- **Actual tokens** (from provider API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Testing: openai\n",
            "============================================================\n",
            "Model: gpt-4o-mini\n",
            "\n",
            "Response: Hello, World!\n",
            "Latency: 1256 ms\n",
            "\n",
            "Token Usage:\n",
            "  Estimated Input:      24 tokens\n",
            "  Actual Prompt:        27 tokens\n",
            "  Actual Completion:     4 tokens\n",
            "  Total (actual):       31 tokens\n",
            "\n",
            "Estimation Accuracy: 88.9%\n",
            "\n",
            "============================================================\n",
            "Testing: google\n",
            "============================================================\n",
            "Model: gemini-2.0-flash-exp\n",
            " Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash-exp\\nPlease retry in 7.101234379s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-exp', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash-exp', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-exp'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '7s'}]}}\n",
            "\n",
            "============================================================\n",
            "Testing: groq\n",
            "============================================================\n",
            "Model: llama-3.1-8b-instant\n",
            "\n",
            "Response: Hello, World!\n",
            "Latency: 193 ms\n",
            "\n",
            "Token Usage:\n",
            "  Estimated Input:      24 tokens\n",
            "  Actual Prompt:        51 tokens\n",
            "  Actual Completion:     5 tokens\n",
            "  Total (actual):       56 tokens\n",
            "\n",
            "Estimation Accuracy: 47.1%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def test_provider(provider_name):\n",
        "    \"\"\"Test a provider with a simple hello world.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {provider_name}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    # Pick appropriate model\n",
        "    model = pick_model(provider_name.lower(), \"general\")\n",
        "    print(f\"Model: {model}\")\n",
        "    \n",
        "    # Create client\n",
        "    client = LLMClient(provider=provider_name.lower(), model=model)\n",
        "    \n",
        "    # Simple message\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Say 'Hello, World!' and nothing else.\"}\n",
        "    ]\n",
        "    \n",
        "    # Call API\n",
        "    try:\n",
        "        response = client.chat(messages, max_tokens=20, temperature=0.0)\n",
        "        \n",
        "        print(f\"\\nResponse: {response['text']}\")\n",
        "        print(f\"Latency: {response['latency_ms']} ms\")\n",
        "        print(f\"\\nToken Usage:\")\n",
        "        print(f\"  Estimated Input:  {response['usage']['input_tokens_est']:>6} tokens\")\n",
        "        print(f\"  Actual Prompt:    {response['usage']['prompt_tokens_actual']:>6} tokens\")\n",
        "        print(f\"  Actual Completion:{response['usage']['completion_tokens_actual']:>6} tokens\")\n",
        "        print(f\"  Total (actual):   {response['usage']['total_tokens_actual']:>6} tokens\")\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        if response['usage']['prompt_tokens_actual']:\n",
        "            est = response['usage']['input_tokens_est']\n",
        "            act = response['usage']['prompt_tokens_actual']\n",
        "            accuracy = (1 - abs(est - act) / act) * 100\n",
        "            print(f\"\\nEstimation Accuracy: {accuracy:.1f}%\")\n",
        "        \n",
        "        # Log to CSV\n",
        "        log_llm_call(\n",
        "            provider=provider_name.lower(),\n",
        "            model=model,\n",
        "            technique=\"hello_world\",\n",
        "            latency_ms=response['latency_ms'],\n",
        "            usage=response['usage'],\n",
        "            retry_count=response['meta']['retry_count'],\n",
        "            backoff_ms_total=response['meta']['backoff_ms_total'],\n",
        "        )\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test all providers that have keys\n",
        "results = {}\n",
        "for provider, key in providers.items():\n",
        "    if key:\n",
        "        provider_name = provider.split()[0].lower()  # \"Google Gemini\" -> \"google\"\n",
        "        if provider_name == \"google\":\n",
        "            provider_name = \"google\"\n",
        "        results[provider] = test_provider(provider_name)\n",
        "    else:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Skipping {provider} (no API key)\")\n",
        "        print('='*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total LLM calls logged: 4\n",
            "\n",
            "Most recent calls:\n",
            "                    timestamp provider                 model    technique  latency_ms  input_tokens_est  context_tokens_est  total_est  prompt_tokens_actual  completion_tokens_actual  total_tokens_actual  retry_count  backoff_ms_total  overflow_handled cost_estimate_usd  notes\n",
            "1  2026-01-15T00:13:04.292217     groq  llama-3.1-8b-instant  hello_world         436                24                   0         27                    51                         5                   56            0                 0             False        ~$0.000003    NaN\n",
            "2  2026-01-15T00:13:48.328290   openai           gpt-4o-mini  hello_world        1256                24                   0         27                    27                         4                   31            0                 0             False        ~$0.000006    NaN\n",
            "3  2026-01-15T00:13:53.013981     groq  llama-3.1-8b-instant  hello_world         193                24                   0         27                    51                         5                   56            0                 0             False        ~$0.000003    NaN\n",
            "\n",
            "============================================================\n",
            "Session Summary:\n",
            "============================================================\n",
            "total_calls: 4\n",
            "avg_latency_ms: 1080.75\n",
            "total_retries: 0\n",
            "techniques_used:\n",
            "  hello_world: 4\n",
            "providers_used:\n",
            "  openai: 2\n",
            "  groq: 2\n",
            "total_prompt_tokens: 156\n",
            "total_completion_tokens: 18\n"
          ]
        }
      ],
      "source": [
        "# View recent logs\n",
        "try:\n",
        "    logs_df = pd.read_csv(\"../logs/runs.csv\")\n",
        "    print(f\"\\nTotal LLM calls logged: {len(logs_df)}\")\n",
        "    print(\"\\nMost recent calls:\")\n",
        "    print(logs_df.tail(3).to_string())\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary = get_log_summary()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Session Summary:\")\n",
        "    print('='*60)\n",
        "    for key, value in summary.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"{key}:\")\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No logs found yet. Run some LLM calls first!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sahas",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
